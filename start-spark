#!/usr/bin/env bash

# Set default HDFS user if not set
if [[ -z "${HDFS_USER}" ]]; then
  export HDFS_USER=spark
fi

if [[ "${1}" = 'master' ]]; then
  # Start Hadoop NameNode
  start-hadoop namenode daemon
  # Start Spark Master
  $SPARK_HOME/sbin/start-master.sh
  tail -f $SPARK_HOME/logs/*

elif [[ "${1}" = 'worker' ]]; then
  # Wait for the Spark Master/Hadoop Namenode to start
  while ! nc -z $2 7077 || ! nc -z $2 50070 ; do
    sleep 2;
  done;
  # Start Hadoop DataNode
  start-hadoop datanode $2 daemon
  # Start Spark Worker
  $SPARK_HOME/sbin/start-slave.sh spark://$2:7077
  tail -f $SPARK_HOME/logs/*

else
  echo "Invalid command '${1}'" >&2
  exit 1
fi

